{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "decd096a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "# Load the dataset\n",
    "dataset_path = 'reverse_image_search.csv'  # Replace with your dataset path\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "# Load the CLIP model and processor\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5584e742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store tensors\n",
    "embeddings = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    image_path = row['path']  # Assuming the path is in a column named 'path'\n",
    "    image = Image.open(image_path).convert('RGB')  # Ensure image is in RGB\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    image_features = model.get_image_features(**inputs)\n",
    "    # Ensure the tensor is detached from the computational graph before converting\n",
    "    embeddings.append(image_features.squeeze(0).detach().numpy().tolist())\n",
    "\n",
    "# Concatenate all feature vectors into a single tensor\n",
    "#image_features_tensor = torch.stack(embeddings)\n",
    "\n",
    "# image_features_tensor now contains the feature vectors for all images in your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "24101874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ebf49137",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection, utility\n",
    "\n",
    "# Milvus parameters\n",
    "HOST = '127.0.0.1'\n",
    "PORT = '19530'\n",
    "TOPK = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4a7d0e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "connections.connect(host=HOST, port=PORT)\n",
    "collection_name = 'tranformers_clip'\n",
    "dim = 512  # Dimension of the embeddings\n",
    "METRIC_TYPE = 'L2'  # You can choose 'L2', 'IP', etc., based on your requirement\n",
    "INDEX_TYPE = 'IVF_FLAT'  # Index type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "934bfb4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text_image_search_blip',\n",
       " 'image_search_collection',\n",
       " 'text_image_search',\n",
       " 'tranformers_clip',\n",
       " 'reverse_image_search']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utility.list_collections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1b6f5a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_milvus_collection(collection_name, dim):\n",
    "    if utility.has_collection(collection_name):\n",
    "        utility.drop_collection(collection_name)\n",
    "    \n",
    "    fields = [\n",
    "        FieldSchema(name='path', dtype=DataType.VARCHAR, description='path to image', max_length=500, \n",
    "                    is_primary=True, auto_id=False),\n",
    "        FieldSchema(name='embedding', dtype=DataType.FLOAT_VECTOR, description='image embedding vectors', dim=dim)\n",
    "    ]\n",
    "    schema = CollectionSchema(fields=fields, description='reverse image search')\n",
    "    collection = Collection(name=collection_name, schema=schema)\n",
    "\n",
    "    index_params = {\n",
    "        'metric_type': METRIC_TYPE,\n",
    "        'index_type': INDEX_TYPE,\n",
    "        'params': {\"nlist\": 512}\n",
    "    }\n",
    "    collection.create_index(field_name='embedding', index_params=index_params)\n",
    "    return collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ef596438",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = create_milvus_collection(collection_name, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3a9f18e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = df['path'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "98ba64a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# embeddings[7] #how an embedding looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "50085e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = [[path for path in paths],\n",
    "            [embedding for embedding in embeddings]]\n",
    "#preparing for insertion to milvus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "080a3e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "mr = collection.insert(entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4aaa48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0fa1d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac86b54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eb18bd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query processing for search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d783ebb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = Collection(collection_name)      # Get an existing collection.\n",
    "collection.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9d241730",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_params = {\n",
    "    \"metric_type\": \"L2\", \n",
    "    \"offset\": 0, \n",
    "    \"ignore_growing\": False, \n",
    "    \"params\": {\"nprobe\": 10}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "eba9ceff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "query_image_path = 'brain_coral.jpg'  \n",
    "query_image = Image.open(query_image_path).convert('RGB')  \n",
    "query_inputs = processor(images=query_image, return_tensors=\"pt\")\n",
    "query_image_features = model.get_image_features(**query_inputs)\n",
    "embedding = query_image_features.squeeze(0).detach().numpy().tolist()\n",
    "\n",
    "\n",
    "# Concatenate all feature vectors into a single tensor\n",
    "#image_features_tensor = torch.stack(embeddings)\n",
    "\n",
    "# image_features_tensor now contains the feature vectors for all images in your dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "64fe84f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = collection.search(\n",
    "    data=[embedding], \n",
    "    anns_field=\"embedding\", \n",
    "    # the sum of `offset` in `param` and `limit` \n",
    "    # should be less than 16384.\n",
    "    param=search_params,\n",
    "    limit=10,\n",
    "    expr=None,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8947ca0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./train/brain_coral/n01917289_2484.JPEG', './train/brain_coral/n01917289_1079.JPEG', './train/brain_coral/n01917289_4021.JPEG', './train/brain_coral/n01917289_765.JPEG', './train/brain_coral/n01917289_1082.JPEG', './train/brain_coral/n01917289_4069.JPEG', './train/brain_coral/n01917289_1783.JPEG', './train/brain_coral/n01917289_1538.JPEG', './train/brain_coral/n01917289_4317.JPEG', './train/brain_coral/n01917289_1022.JPEG']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0].ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "99b6bf12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[23.210317611694336, 27.270641326904297, 34.20412826538086, 34.476402282714844, 42.19493103027344, 43.957298278808594, 44.653133392333984, 54.235069274902344, 62.594215393066406, 73.49400329589844]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0].distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86bbe21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c7f61b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3d75c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef42258",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8137f5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
