{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e712e56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "# Load the dataset\n",
    "dataset_path = 'reverse_image_search.csv'  # Replace with your dataset path\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "# Load the CLIP model and processor\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcc189c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store tensors\n",
    "embeddings = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    image_path = row['path']  # Assuming the path is in a column named 'path'\n",
    "    image = Image.open(image_path).convert('RGB')  # Ensure image is in RGB\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    image_features = model.get_image_features(**inputs)\n",
    "    # Ensure the tensor is detached from the computational graph before converting\n",
    "    embeddings.append(image_features.squeeze(0).detach().numpy().tolist())\n",
    "\n",
    "# Concatenate all feature vectors into a single tensor\n",
    "#image_features_tensor = torch.stack(embeddings)\n",
    "\n",
    "# image_features_tensor now contains the feature vectors for all images in your dataset>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17ecc8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection, utility\n",
    "\n",
    "# Milvus parameters\n",
    "HOST = '127.0.0.1'\n",
    "PORT = '19530'\n",
    "TOPK = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99d91dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "connections.connect(host=HOST, port=PORT)\n",
    "collection_name = 'tranformers_clip_patch16'\n",
    "dim = 512  # Dimension of the embeddings\n",
    "METRIC_TYPE = 'L2'  # You can choose 'L2', 'IP', etc., based on your requirement\n",
    "INDEX_TYPE = 'IVF_FLAT'  # Index type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "407dc8fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utility.list_collections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e4df2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "utility.drop_collection(\"tranformers_clip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a334cd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_milvus_collection(collection_name, dim):\n",
    "    if utility.has_collection(collection_name):\n",
    "        utility.drop_collection(collection_name)\n",
    "    \n",
    "    fields = [\n",
    "        FieldSchema(name='path', dtype=DataType.VARCHAR, description='path to image', max_length=500, \n",
    "                    is_primary=True, auto_id=False),\n",
    "        FieldSchema(name='embedding', dtype=DataType.FLOAT_VECTOR, description='image embedding vectors', dim=dim)\n",
    "    ]\n",
    "    schema = CollectionSchema(fields=fields, description='reverse image search')\n",
    "    collection = Collection(name=collection_name, schema=schema)\n",
    "\n",
    "    index_params = {\n",
    "        'metric_type': METRIC_TYPE,\n",
    "        'index_type': INDEX_TYPE,\n",
    "        'params': {\"nlist\": 512}\n",
    "    }\n",
    "    collection.create_index(field_name='embedding', index_params=index_params)\n",
    "    return collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "72ce79d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = create_milvus_collection(collection_name, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce31c931",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "90456657",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = df['path'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "39f4d39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = [[path for path in paths],\n",
    "            [embedding for embedding in embeddings]]\n",
    "#preparing for insertion to milvus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "d10bf3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mr = collection.insert(entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825c8d51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a820f7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "336efe6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = Collection(collection_name)      # Get an existing collection.\n",
    "collection.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "963ba23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_params = {\n",
    "    \"metric_type\": \"L2\", \n",
    "    \"offset\": 0, \n",
    "    \"ignore_growing\": False, \n",
    "    \"params\": {\"nprobe\": 10}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "a34b6d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# search with image\n",
    "\n",
    "query_image_path = 'aleren.jpeg'  \n",
    "query_image = Image.open(query_image_path).convert('RGB')  \n",
    "query_inputs = processor(images=query_image, return_tensors=\"pt\")\n",
    "query_image_features = model.get_image_features(**query_inputs)\n",
    "embedding = query_image_features.squeeze(0).detach().numpy().tolist()\n",
    "\n",
    "\n",
    "# Concatenate all feature vectors into a single tensor\n",
    "#image_features_tensor = torch.stack(embeddings)\n",
    "\n",
    "# image_features_tensor now contains the feature vectors for all images in your dataset\n",
    "\n",
    "results = collection.search(\n",
    "    data=[embedding], \n",
    "    anns_field=\"embedding\", \n",
    "    # the sum of `offset` in `param` and `limit` \n",
    "    # should be less than 16384.\n",
    "    param=search_params,\n",
    "    limit=10,\n",
    "    expr=None,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "27076399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./train/basketball/n02802426_7656.JPEG',\n",
       " './train/basketball/n02802426_24958.JPEG',\n",
       " './train/horizontal_bar/n03535780_16077.JPEG',\n",
       " './train/horizontal_bar/n03535780_18270.JPEG',\n",
       " './train/basketball/n02802426_12782.JPEG',\n",
       " './train/basketball/n02802426_9952.JPEG',\n",
       " './train/basketball/n02802426_26718.JPEG',\n",
       " './train/basketball/n02802426_7726.JPEG',\n",
       " './train/basketball/n02802426_3881.JPEG',\n",
       " './train/ski_mask/n04229816_6821.JPEG']"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0].ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "d0a21a5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[93.42448425292969,\n",
       " 93.95035552978516,\n",
       " 96.57243347167969,\n",
       " 102.10467529296875,\n",
       " 102.97093963623047,\n",
       " 104.1107177734375,\n",
       " 105.00439453125,\n",
       " 105.82638549804688,\n",
       " 106.10682678222656,\n",
       " 106.15518188476562]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0].distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "3d1cfb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search with text\n",
    "query_text = \"airplane\"  \n",
    "text_inputs = processor(text=query_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=77)\n",
    "query_text_features = model.get_text_features(**text_inputs)\n",
    "text_embedding = query_text_features.squeeze(0).detach().numpy().tolist()\n",
    "\n",
    "results = collection.search(\n",
    "    data=[text_embedding], \n",
    "    anns_field=\"embedding\", \n",
    "    # the sum of `offset` in `param` and `limit` \n",
    "    # should be less than 16384.\n",
    "    param=search_params,\n",
    "    limit=10,\n",
    "    expr=None,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "86f4b216",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./train/comic_book/n06596364_19168.JPEG',\n",
       " './train/bottlecap/n02877765_1596.JPEG',\n",
       " './train/safety_pin/n04127249_5909.JPEG',\n",
       " './train/harmonica/n03494278_30921.JPEG',\n",
       " './train/warplane/n04552348_16150.JPEG',\n",
       " './train/warplane/n04552348_12780.JPEG',\n",
       " './train/dishwasher/n03207941_15169.JPEG',\n",
       " './train/warplane/n04552348_10736.JPEG',\n",
       " './train/warplane/n04552348_2027.JPEG',\n",
       " './train/magpie/n01582220_10712.JPEG']"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0].ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "04f6da38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "(eog:99022): EOG-CRITICAL **: 15:28:33.580: eog_image_get_file: assertion 'EOG_IS_IMAGE (img)' failed\n",
      "\n",
      "(eog:99022): GLib-GIO-CRITICAL **: 15:28:33.580: g_file_equal: assertion 'G_IS_FILE (file1)' failed\n",
      "\n",
      "(eog:99022): EOG-CRITICAL **: 15:28:33.580: eog_image_get_file: assertion 'EOG_IS_IMAGE (img)' failed\n",
      "\n",
      "(eog:99022): GLib-GIO-CRITICAL **: 15:28:33.580: g_file_equal: assertion 'G_IS_FILE (file1)' failed\n",
      "\n",
      "(eog:99022): EOG-CRITICAL **: 15:28:33.580: eog_image_get_file: assertion 'EOG_IS_IMAGE (img)' failed\n",
      "\n",
      "(eog:99022): GLib-GIO-CRITICAL **: 15:28:33.580: g_file_equal: assertion 'G_IS_FILE (file1)' failed\n",
      "\n",
      "(eog:99022): EOG-CRITICAL **: 15:28:33.651: eog_image_get_file: assertion 'EOG_IS_IMAGE (img)' failed\n",
      "\n",
      "(eog:99022): GLib-GIO-CRITICAL **: 15:28:33.651: g_file_equal: assertion 'G_IS_FILE (file1)' failed\n",
      "\n",
      "(eog:99022): EOG-CRITICAL **: 15:28:33.651: eog_image_get_file: assertion 'EOG_IS_IMAGE (img)' failed\n",
      "\n",
      "(eog:99022): GLib-GIO-CRITICAL **: 15:28:33.651: g_file_equal: assertion 'G_IS_FILE (file1)' failed\n",
      "\n",
      "(eog:99022): EOG-CRITICAL **: 15:28:33.651: eog_image_get_file: assertion 'EOG_IS_IMAGE (img)' failed\n",
      "\n",
      "(eog:99022): GLib-GIO-CRITICAL **: 15:28:33.651: g_file_equal: assertion 'G_IS_FILE (file1)' failed\n",
      "\n",
      "(eog:99022): EOG-CRITICAL **: 15:28:33.813: eog_image_get_file: assertion 'EOG_IS_IMAGE (img)' failed\n",
      "\n",
      "(eog:99022): GLib-GIO-CRITICAL **: 15:28:33.813: g_file_equal: assertion 'G_IS_FILE (file1)' failed\n",
      "\n",
      "(eog:99022): EOG-CRITICAL **: 15:28:33.813: eog_image_get_file: assertion 'EOG_IS_IMAGE (img)' failed\n",
      "\n",
      "(eog:99022): GLib-GIO-CRITICAL **: 15:28:33.813: g_file_equal: assertion 'G_IS_FILE (file1)' failed\n",
      "\n",
      "(eog:99022): EOG-CRITICAL **: 15:28:33.861: eog_image_get_file: assertion 'EOG_IS_IMAGE (img)' failed\n",
      "\n",
      "(eog:99022): GLib-GIO-CRITICAL **: 15:28:33.861: g_file_equal: assertion 'G_IS_FILE (file1)' failed\n",
      "\n",
      "(eog:99022): EOG-CRITICAL **: 15:28:33.861: eog_image_get_file: assertion 'EOG_IS_IMAGE (img)' failed\n",
      "\n",
      "(eog:99022): GLib-GIO-CRITICAL **: 15:28:33.861: g_file_equal: assertion 'G_IS_FILE (file1)' failed\n",
      "\n",
      "(eog:99022): EOG-CRITICAL **: 15:28:35.923: eog_image_get_file: assertion 'EOG_IS_IMAGE (img)' failed\n",
      "\n",
      "(eog:99022): GLib-GIO-CRITICAL **: 15:28:35.923: g_file_equal: assertion 'G_IS_FILE (file1)' failed\n"
     ]
    }
   ],
   "source": [
    "for result_path in results[0].ids:\n",
    "    result_image = Image.open(result_path).convert('RGB')\n",
    "    result_image.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "286171ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[154.8622589111328,\n",
       " 154.98194885253906,\n",
       " 156.00918579101562,\n",
       " 157.588134765625,\n",
       " 158.6085968017578,\n",
       " 158.73123168945312,\n",
       " 159.7007293701172,\n",
       " 159.90402221679688,\n",
       " 160.07493591308594,\n",
       " 160.310546875]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0].distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c670482",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:milvus]",
   "language": "python",
   "name": "conda-env-milvus-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
